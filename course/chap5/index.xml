<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Chapter 5 | ロボットシステム入門</title><link>https://matsuolab.github.io/roomba_hack/course/chap5/</link><atom:link href="https://matsuolab.github.io/roomba_hack/course/chap5/index.xml" rel="self" type="application/rss+xml"/><description>Chapter 5</description><generator>Wowchemy (https://wowchemy.com)</generator><language>ja</language><copyright>© 2022 Tokyo Robot And Intelligence Lab (TRAIL)</copyright><lastBuildDate>Sun, 09 Sep 2018 00:00:00 +0000</lastBuildDate><image><url>https://matsuolab.github.io/roomba_hack/media/icon_hua2ec155b4296a9c9791d015323e16eb5_11927_512x512_fill_lanczos_center_2.png</url><title>Chapter 5</title><link>https://matsuolab.github.io/roomba_hack/course/chap5/</link></image><item><title>三次元画像処理</title><link>https://matsuolab.github.io/roomba_hack/course/chap5/three-dimensions/</link><pubDate>Sat, 22 Jan 2022 00:00:00 +0000</pubDate><guid>https://matsuolab.github.io/roomba_hack/course/chap5/three-dimensions/</guid><description>&lt;h2 id="learn">Learn&lt;/h2>
&lt;p>今回はRealSenseD435というRGBDカメラを用いて三次元画像処理を行っていきましょう。&lt;/p>
&lt;h3 id="rgbdカメラについて">RGBDカメラについて&lt;/h3>
&lt;p>RGBDカメラとは、カラーの他にデプス(深度)を取得できるカメラのことです。
複雑な動作を行うロボットを動かす際には三次元空間の把握が重要となり、RGBDカメラはよく用いられます。
比較的安価でよく利用されるRGBDカメラとして、Intel社製のRealSenseやMicrosoft社製のXtionなどがあります。&lt;/p>
&lt;h3 id="realsense">RealSense&lt;/h3>
&lt;p>今回はRGBDカメラとしてRealSenseD435を使用します。&lt;/p>
&lt;p>ROSで用いる際には標準のラッパー(&lt;a href="https://github.com/IntelRealSense/realsense-ros">https://github.com/IntelRealSense/realsense-ros&lt;/a>)を使用します。&lt;/p>
&lt;p>&lt;code>roslaunch realsense2_camera rs_camera.launch&lt;/code>を行うとデフォルトのトピックとして
RGB画像の&lt;code>/camera/color/image_raw&lt;/code>、
デプス画像の&lt;code>/camera/depth/image_raw&lt;/code>
が利用できます。これらのトピックはいずれも&lt;code>sensor_msgs/Image&lt;/code>型です。&lt;/p>
&lt;p>RealSenseは物理的にRGB画像モジュールとデプス画像モジュールが離れているため、これら2つのトピックはいずれも画像データではあるものの、ピクセルの位置関係が対応しておらずそのままだとうまく画像処理に用いることができません。
そこで、起動時に&lt;code>align:=true&lt;/code>を指定することで、上記のトピックに加えてデプス画像をRGB画像のピクセルに対応するように変換する&lt;code>/camera/aligned_depth_to_color/image_raw&lt;/code>トピックを使用できるようにします。
他にも&lt;code>pointcloud:=true&lt;/code>を指定するとデプス画像から点群を生成することができます。
しかし、この処理は比較的重たいため今回はJetsonではなく、開発用PCでこの処理を行っていくことにします。&lt;/p>
&lt;p>それでは、RGB画像&lt;code>/camera/color/image_raw&lt;/code>と整列されたデプス画像&lt;code>/camera/aligned_depth_to_color/image_raw&lt;/code>の2種類のトピックを用いて三次元画像処理を行っていきましょう。&lt;/p>
&lt;h3 id="物体検出">物体検出&lt;/h3>
&lt;p>まずはRGB画像&lt;code>/camera/color/image_raw&lt;/code>のみを用いて三次元ではない画像検出を行っていきましょう。&lt;/p>
&lt;p>以下は&lt;code>/camera/color/image_raw&lt;/code>をSubscribeし、物体検出アルゴリズムであるYOLOv3に入力し、その結果をbounding boxとして描画し、&lt;code>/detection_result&lt;/code>としてPublishするスクリプトです。&lt;/p>
&lt;pre>&lt;code>#!/usr/bin/env python3
import rospy
from sensor_msgs.msg import Image
from cv_bridge import CvBridge
from pytorchyolo import detect, models
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import cv2
import copy
class ObjectDetection:
def __init__(self):
rospy.init_node('object_detection', anonymous=True)
# Publisher
self.detection_result_pub = rospy.Publisher('/detection_result', Image, queue_size=10)
# Subscriber
rgb_sub = rospy.Subscriber('/camera/color/image_raw', Image, self.callback_rgb)
self.bridge = CvBridge()
self.rgb_image = None
def callback_rgb(self, data):
cv_array = self.bridge.imgmsg_to_cv2(data, 'bgr8')
cv_array = cv2.cvtColor(cv_array, cv2.COLOR_BGR2RGB)
self.rgb_image = cv_array
def process(self):
path = &amp;quot;/root/roomba_hack/catkin_ws/src/three-dimensions_tutorial/yolov3/&amp;quot;
# load category
with open(path+&amp;quot;data/coco.names&amp;quot;) as f:
category = f.read().splitlines()
# prepare model
model = models.load_model(path+&amp;quot;config/yolov3.cfg&amp;quot;, path+&amp;quot;weights/yolov3.weights&amp;quot;)
while not rospy.is_shutdown():
if self.rgb_image is None:
continue
# inference
tmp_image = copy.copy(self.rgb_image)
boxes = detect.detect_image(model, tmp_image)
# [[x1, y1, x2, y2, confidence, class]]
# plot bouding box
for box in boxes:
x1, y1, x2, y2 = map(int, box[:4])
cls_pred = int(box[5])
tmp_image = cv2.rectangle(tmp_image, (x1, y1), (x2, y2), (0, 255, 0), 3)
tmp_image = cv2.putText(tmp_image, category[cls_pred], (x1, y1), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 2)
# publish image
tmp_image = cv2.cvtColor(tmp_image, cv2.COLOR_RGB2BGR)
detection_result = self.bridge.cv2_to_imgmsg(tmp_image, &amp;quot;bgr8&amp;quot;)
self.detection_result_pub.publish(detection_result)
if __name__ == '__main__':
od = ObjectDetection()
try:
od.process()
except rospy.ROSInitException:
pass
&lt;/code>&lt;/pre>
&lt;p>コールバック関数で&lt;code>sensor_msgs/Image&lt;/code>型をnp.ndarray型に変換するために&lt;/p>
&lt;pre>&lt;code>cv_array = self.bridge.imgmsg_to_cv2(data, 'bgr8')
cv_array = cv2.cvtColor(cv_array, cv2.COLOR_BGR2RGB)
&lt;/code>&lt;/pre>
&lt;p>という&lt;code>sensor_msgs/Image&lt;/code>型特有の処理を行ってますが、Subscriberを作成しコールバック関数でデータを受け取るという基本的な処理の流れは&lt;code>scan&lt;/code>などの他のセンサと同じです。&lt;/p>
&lt;p>ここで注意してほしいのはYOLOの推論部分をコールバック関数内で行っていないことです。
一見、新しいデータが入ってくるときのみに推論を回すことは合理的に見えますが、センサの入力に対してコールバック関数内の処理が重いとセンサの入力がどんどん遅れていってしまいます。
コールバック関数内ではセンサデータの最低限の処理の記述にとどめ、重い処理は分けて書くことを意識しましょう。&lt;/p>
&lt;p>ここでは既存の物体検出モジュールを使用しましたが、PyTorchなどで作成した自作のモデルも同様の枠組みで利用することができます。&lt;/p>
&lt;p>続いて、デプス画像データも統合して物体を検出し、物体までの距離を測定してみましょう。&lt;/p>
&lt;h3 id="外部パッケージの使用">外部パッケージの使用&lt;/h3>
&lt;p>それではデプス画像からを作成しましょう。&lt;/p>
&lt;h2 id="演習">演習&lt;/h2>
&lt;!-- &lt;details class="spoiler " id="spoiler-0">
&lt;summary>Dockerfileにamclを追加してBuildする&lt;/summary>
&lt;p>&lt;/p>
&lt;/details> -->
&lt;details class="spoiler " id="spoiler-1">
&lt;summary>ブランチの切り替え&lt;/summary>
&lt;p>&lt;pre>&lt;code>(jetson, 開発PC) git fetch
(jetson, 開発PC) git checkout feature/move-base
&lt;/code>&lt;/pre>
&lt;/p>
&lt;/details>
&lt;details class="spoiler " id="spoiler-2">
&lt;summary>(開発PC, jetson)起動準備&lt;/summary>
&lt;p>&lt;pre>&lt;code>(jetson)./RUN-DOCKER-CONTAINER.sh
(jetson)(docker) roslaunch roomba_bringup bringup.launch
(開発PC)./RUN-DOCKER-CONTAINER.sh 192.168.10.7x
&lt;/code>&lt;/pre>
&lt;/p>
&lt;/details>
&lt;details class="spoiler " id="spoiler-3">
&lt;summary>gmappingで地図作成&lt;/summary>
&lt;p>&lt;pre>&lt;code>(docker) roslaunch navigation_tutorial gmapping.launch
&lt;/code>&lt;/pre>
&lt;p>地図の保存。map.pgm（画像データ）とmap.yaml(地図情報)が保存される。&lt;/p>
&lt;pre>&lt;code>(docker) rosrun map_server map_saver
&lt;/code>&lt;/pre>
&lt;p>&lt;code>~/roomba_hack/catkin_ws/src/navigation_tutorial/map&lt;/code> の下に保存する。&lt;/p>
&lt;/p>
&lt;/details>
&lt;details class="spoiler " id="spoiler-4">
&lt;summary>amclをlaunchして、自己位置推定する&lt;/summary>
&lt;p>&lt;p>localizationノードと地図サーバーを同時に起動。&lt;/p>
&lt;pre>&lt;code>(docker) roslaunch navigation_tutorial localization.launch
(docker) roslaunch roomba_teleop teleop.launch
(docker) rviz -d /root/roomba_hack/catkin_ws/src/navigation_tutorial/configs/navigation.rviz
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>初期位置の指定(rvizの2D Pose Estimate)&lt;/li>
&lt;li>コントローラで移動させてみて自己位置を確認&lt;/li>
&lt;li>rqt_tf_treeを見てみる&lt;/li>
&lt;/ul>
&lt;/p>
&lt;/details>
&lt;details class="spoiler " id="spoiler-5">
&lt;summary>amclのparamをチューニングする&lt;/summary>
&lt;p>&lt;p>launchファイルの中身を見てみて、値を変えてみる。&lt;/p>
&lt;p>各パラメータの意味は&lt;a href="https://wiki.ros.org/amcl#Parameters">amclのページ&lt;/a>を参照。&lt;/p>
&lt;p>例えば、・・・&lt;/p>
&lt;ul>
&lt;li>initial_cov_**を大きくしてみて、パーティクルがちゃんと収束するかみてみる。&lt;/li>
&lt;li>particleの数(min_particles、max_particles)を変えてみて挙動をみてみる。&lt;/li>
&lt;/ul>
&lt;/p>
&lt;/details></description></item></channel></rss>